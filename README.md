# CMPE-257-Fall23-Tianzheng-Cui1



https://colab.research.google.com/drive/1oJBC4wyirIzNAHSh-hohBx_RLCcPkvVU#scrollTo=IHnwOEhfOjDG

In conclusion, I found that for Problem 1.5, (a, b, c), the output of the algorithm came up very ridiculous slopes and intercepts, and according to the list that conatin error elements, my algorithm believed that there is no error after 1000 updates(as for when the learning rate is 0.01, it believed that there is one error, but abvious there is much more than one error). This might because the way the algorithm check errors is based on y(t)*s(t) instead of checking whether the result generated by activation function matches the given y[], this is why it generated weights that obviously ridiculous, even though the algorithm believed that there will be no errors by applying the ridicious weights.
But as I can see, the lower the learning rate, the closer line will be generate. When the learningrate is 0.0001, the result is much more closer to correct weights.
